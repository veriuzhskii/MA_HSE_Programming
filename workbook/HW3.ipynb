{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Загрузка данных**"
      ],
      "metadata": {
        "id": "VyHHlYhjcj77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загружаем тренировочные данные\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/data_cleaning.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mEhxQY1_q0I",
        "outputId": "f1319444-ebd5-4ab5-d162-fbd46a9a8da3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-05 15:43:52--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/data_cleaning.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2377 (2.3K) [text/plain]\n",
            "Saving to: ‘data_cleaning.txt’\n",
            "\n",
            "data_cleaning.txt   100%[===================>]   2.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-05 15:43:53 (41.4 MB/s) - ‘data_cleaning.txt’ saved [2377/2377]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae588432-63f0-4bd4-c95a-5ac306c6566b",
        "id": "CUh9kwrucj78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html>\n",
            "<head>\n",
            "    <title>Sample HTML Document</title>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Welcome\n"
          ]
        }
      ],
      "source": [
        "# запишем данные в переменную data\n",
        "with open('data_cleaning.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# выведите на экран первые 100 символов с помощью слайсинга\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Удаление артефактов**"
      ],
      "metadata": {
        "id": "DYrLLtAeBoHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # загрузим библиотеку для обработки регулярных выражений"
      ],
      "metadata": {
        "id": "X8-ttaVkByEt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07914b7c-e1d1-42aa-e38c-159a0e4977ea",
        "id": "eKTouY2aBtTN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's crucial to handle HTML entities such as &lt;div&gt;, &lt;p&gt;, &amp;, etc\n"
          ]
        }
      ],
      "source": [
        "# пропишем паттерн для поиска HTML-тегов вида  ...\n",
        "\n",
        "tag_pattern = r'<[^>]+>' # паттерн для поиска тегов\n",
        "\n",
        "clean_text = re.sub(tag_pattern, \"\", data)\n",
        "\n",
        "# текст с 720-го по 800-ый символ\n",
        "print(clean_text[720:800])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbols_pattern = r'&\\w+;' # паттерн для поиска специальных символов\n",
        "\n",
        "clean_text = re.sub(symbols_pattern, \"\", clean_text)\n",
        "print(clean_text[720:800])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwT6gXs0COcK",
        "outputId": "ccec7030-4bb9-4adc-f4cd-d0656efe6685"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's crucial to handle HTML entities such as div, p, , etc. HTML entities are s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# выведите на экран первые 100 символов вашего текущего результата\n",
        "# на этот раз не используйте print\n",
        "clean_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ck340iFsHwbL",
        "outputId": "48e485af-0b68-49cf-c757-fe0cf656d7be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n    Sample HTML Document\\n\\n\\n    Welcome to Data Cleaning\\n    This is a sample paragraph with HTML '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "space_pattern = r'\\s+'\n",
        "\n",
        "clean_text = re.sub(space_pattern, \" \", clean_text) # примените re.sub\n",
        "clean_text[:100] # выведите первые 100 символов, не используя print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RAGyngSzICxk",
        "outputId": "501f54ba-4d63-4df4-ae4a-e60f4c96cce3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sample HTML Document Welcome to Data Cleaning This is a sample paragraph with HTML artifacts such a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Смена регистра**"
      ],
      "metadata": {
        "id": "tpnDPnTDJFtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_lower = clean_text.lower() # примените lower к clean_text ###\n",
        "text_lower[-100:]# выведите первые 100 символов с конца, используйте слайсинг и не забудьте про -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U-wYnJtOJFtl",
        "outputId": "abed8249-54e9-43c0-87ac-a73951048191"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e learning models, making predictions, or generating insights to support decision-making processes. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Удаление стоп-слов**"
      ],
      "metadata": {
        "id": "58J-hpb2Ksue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "JVlBlyDwMKw1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nIIBrEgKxXf",
        "outputId": "c235a5d2-31d5-4eb1-9136-7bca582742f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-05 16:31:53--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 954 [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     954  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-05 16:31:54 (28.7 MB/s) - ‘stopwords.txt’ saved [954/954]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в переменную stopwords\n",
        "with open('stopwords.txt', 'r') as f:\n",
        "  stopwords = f.read().split()\n",
        "\n",
        "# выведите на экран первые 10 стоп-слов\n",
        "stopwords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4848a6a5-4369-4a52-9994-1ced9b409c48",
        "id": "YSyNuN8dKsue"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_word = random.choice(stopwords)\n",
        "print(f\"Результат проверки: {random_word in text_lower}\\nСлучайное слово: {random_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAK5BM1dNQp9",
        "outputId": "b84b687e-7c22-4e3e-e230-87abaf25dfe7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат проверки: False\n",
            "Случайное слово: too\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Токенизация**"
      ],
      "metadata": {
        "id": "NixgGlJdP4m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_lower.split('.') # split для сегментации по знаку `.`\n",
        "sentences[:10] # вывод на экран первых 10 предложений"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQyKVZPsP4m6",
        "outputId": "82e9a049-dfeb-4536-cac6-7eab0297206e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' sample html document welcome to data cleaning this is a sample paragraph with html artifacts such as bold and italic text',\n",
              " ' data cleaning is an essential process in preparing data for analysis',\n",
              " ' it involves various techniques to clean, transform, and preprocess data',\n",
              " ' in data cleaning, it\\'s important to remove stop words like \"the\", \"and\", \"of\", etc',\n",
              " ' stop words are common words that are often filtered out from text data because they do not carry significant meaning',\n",
              " \" here's another paragraph\",\n",
              " ' sometimes text is in uppercase or lowercase',\n",
              " \" it's important to standardize the text case to ensure consistency in the dataset\",\n",
              " ' this can be achieved by converting all text to lowercase or uppercase',\n",
              " \" it's crucial to handle html entities such as div, p, , etc\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text_lower.split() # split для сегментации по пробелу\n",
        "print(tokens[:10]) # вывод на экран первых 10 слов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7-lzcN7QslA",
        "outputId": "11296681-745a-48b3-f2ce-8e23259368ec"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'html', 'document', 'welcome', 'to', 'data', 'cleaning', 'this', 'is', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = []\n",
        "\n",
        "for token in tokens:  # итерация по списку токенов tokens\n",
        "  if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "    clean_tokens.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "\n",
        "# вывод на экран первых 10 элементов clean_tokens\n",
        "print(clean_tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azO6WV2ARn_d",
        "outputId": "b2007e35-703b-470d-bf19-57f8cd2573aa"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'html', 'document', 'welcome', 'data', 'cleaning', 'sample', 'paragraph', 'html', 'artifacts']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **И еще одно задание...**"
      ],
      "metadata": {
        "id": "AOGZNo-3SU4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/extracurricular/artefacts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40138ed0-a41b-4ba4-f318-2ad9c29a0291",
        "id": "UA0VMCMVSU4v"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-05 17:05:20--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/extracurricular/artefacts.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845 [text/plain]\n",
            "Saving to: ‘artefacts.txt’\n",
            "\n",
            "artefacts.txt       100%[===================>]     845  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-05 17:05:20 (32.4 MB/s) - ‘artefacts.txt’ saved [845/845]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в переменную artefacts\n",
        "with open('artefacts.txt', 'r') as f:\n",
        "  artefacts = f.read()"
      ],
      "metadata": {
        "id": "X23YIcFmSdZC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# удалить артефакты (html-теги, специальные символы и двойные пробелы)\n",
        "tag_pattern = r'<[^>]+>'\n",
        "symbols_pattern = r'&\\w+;'\n",
        "space_pattern = r'\\s+'\n",
        "\n",
        "patterns = [tag_pattern, symbols_pattern, space_pattern]\n",
        "\n",
        "for pattern in patterns:\n",
        "    clean_text = re.sub(pattern, \" \", clean_text)\n",
        "\n",
        "# привести текст к нижнему регистру\n",
        "text_lower = clean_text.lower()\n",
        "\n",
        "# токенизировать текст по предложениям\n",
        "sentences = text_lower.split('.')\n",
        "\n",
        "# токенизировать текст по словам\n",
        "tokens = text_lower.split()\n",
        "\n",
        "# удалить стоп-слова\n",
        "clean_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "  if token not in stopwords:\n",
        "    clean_tokens.append(token)\n",
        "\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UN7cO_1Sd66",
        "outputId": "65e36af5-6771-4550-d830-0839328ae207"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'html', 'document', 'welcome', 'data', 'cleaning', 'sample', 'paragraph', 'html', 'artifacts', 'bold', 'italic', 'text.', 'data', 'cleaning', 'essential', 'process', 'preparing', 'data', 'analysis.', 'involves', 'various', 'techniques', 'clean,', 'transform,', 'preprocess', 'data.', 'data', 'cleaning,', 'important', 'remove', 'stop', 'words', 'like', '\"the\",', '\"and\",', '\"of\",', 'etc.', 'stop', 'words', 'common', 'words', 'often', 'filtered', 'text', 'data', 'carry', 'significant', 'meaning.', 'another', 'paragraph.', 'sometimes', 'text', 'uppercase', 'lowercase.', 'important', 'standardize', 'text', 'case', 'ensure', 'consistency', 'dataset.', 'can', 'achieved', 'converting', 'text', 'lowercase', 'uppercase.', 'crucial', 'handle', 'html', 'entities', 'div,', 'p,', ',', 'etc.', 'html', 'entities', 'special', 'characters', 'symbols', 'specific', 'meanings', 'html.', 'need', 'properly', 'handled', 'avoid', 'issues', 'data', 'processing.', 'data', 'cleaning', 'also', 'involves', 'dealing', 'missing', 'values.', 'missing', 'values', 'can', 'occur', 'due', 'various', 'reasons', 'incomplete', 'data', 'collection', 'data', 'entry', 'errors.', 'essential', 'identify', 'handle', 'missing', 'values', 'appropriately', 'avoid', 'bias', 'analysis.', 'text', 'data', 'often', 'contains', 'noise', 'punctuation', 'marks,', 'special', 'characters,', 'digits.', 'removing', 'noise', 'text', 'data', 'necessary', 'focus', 'meaningful', 'content.', 'techniques', 'regular', 'expressions', 'can', 'used', 'noise', 'removal.', 'another', 'important', 'aspect', 'data', 'cleaning', 'deduplication.', 'duplicate', 'records', 'dataset', 'can', 'skew', 'analysis', 'results', 'lead', 'inaccurate', 'conclusions.', 'identifying', 'removing', 'duplicate', 'records', 'ensures', 'data', 'integrity', 'improves', 'quality', 'analysis.', 'cleaning', 'data,', 'essential', 'perform', 'exploratory', 'data', 'analysis', '(eda)', 'gain', 'insights', 'identify', 'patterns.', 'eda', 'involves', 'visualizing', 'data,', 'computing', 'summary', 'statistics,', 'exploring', 'relationships', 'variables.', 'data', 'cleaned', 'analyzed,', 'can', 'used', 'various', 'purposes', 'building', 'machine', 'learning', 'models,', 'making', 'predictions,', 'generating', 'insights', 'support', 'decision-making', 'processes.']\n"
          ]
        }
      ]
    }
  ]
}